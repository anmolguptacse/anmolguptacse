# Hi, I'm Anmol Gupta ğŸ‘‹

ğŸ“ M.Tech CSE @ IISc Bangalore (2025â€“27)

---

## ğŸš€ Interests

- ğŸ¤– Machine Learning & Deep Learning
- ğŸ§  Transformers, Seq2Seq, Attention, Decoding
- âš™ï¸ Systems, Parallelism, GPU Computing (CUDA)

---

## ğŸ› ï¸ Skills

**Languages:** C++, Python, CUDA  
**ML/DL:** PyTorch, Transformers, Seq2Seq, Attention, Evaluation Metrics  
**Systems:** GPU Optimization, Memory Hierarchy, Nsight Profiling, Parallelism  

---

## ğŸ“Œ Featured Projects

### ğŸ”¹ [Translating Indian Names (Seq2Seq + Attention)](https://github.com/anmolguptacse/Translating-Indian-names)
- Character-level encoderâ€“decoder with attention and custom Hindi tokenizer
- Implemented greedy and beam search decoding
- Achieved **92% accuracy, 88 BLEU, 4% CER**

---

### ğŸ”¹ [Transformer Implementation From Scratch](https://github.com/anmolguptacse/Transformer-Implementation-From-Scratch-)
- Built multi-head attention, positional encoding, FFN, and layer norm from scratch
- Trained on OPUS Englishâ€“French dataset
- Visualized attention maps for interpretability

---

### ğŸ”¹ Personalized Cancer Diagnosis (ML + Ensembling)
- Multi-class classification using gene mutation and clinical text data
- Benchmarked NB, LR, SVM, RF and built stacking/voting ensembles

---

### ğŸ”¹ [GPU-Accelerated 2D Convolution Optimization on NVIDIA A100](https://github.com/anmolguptacse/GPU-Accelerated-2D-Convolution-Optimization-on-NVIDIA-A100)
- Achieved **15.8Ã— speedup** using memory coalescing, shared memory tiling, ILP, and multi-stream concurrency
- Converted memory-bound kernel to compute-bound (16% â†’ 93% utilization)
- Optimized systematically using Nsight Compute
